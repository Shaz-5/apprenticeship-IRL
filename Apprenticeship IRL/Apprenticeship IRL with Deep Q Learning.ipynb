{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643f3587",
   "metadata": {},
   "source": [
    "# Apprenticeship Learning via IRL - Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495553a8",
   "metadata": {},
   "source": [
    "## Cartpole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080aee0",
   "metadata": {},
   "source": [
    "[CartPole-v0 Wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932e51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import argparse\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae693d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to store the experiences (transitions) of the agent as it interacts with the environment\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity   # maximum number of transitions the buffer can hold\n",
    "        self.memory = []           # list to store the transitions\n",
    "        self.position = 0          # counter to keep track of the next available slot in the buffer\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Save a transition in the replay memory.\n",
    "\n",
    "        Parameters:\n",
    "        - *args: Tuple representing a transition.\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.memory) < self.capacity:                  # if buffer not full, appends the transition\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)        # overwrites the transition at the current position\n",
    "        self.position = (self.position + 1) % self.capacity   # update counter to the next available slot\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the replay memory.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the replay memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4995c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition in the replay memory\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f21d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for Deep Q Learning \n",
    "\n",
    "HIDDEN_LAYER = 64\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DQN model.\n",
    "\n",
    "        The model consists of three hidden layers with ReLU activation functions.\n",
    "\n",
    "        Input:\n",
    "        - 4 input features (state space)\n",
    "        - HIDDEN_LAYER number of neurons in each hidden layer\n",
    "        \n",
    "        Output:\n",
    "          - 2 output neurons representing Q-values for each action\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.il = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.h1 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.h2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        # self.h3 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.ol = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input state.\n",
    "\n",
    "        Returns:\n",
    "        - Output Q-values for each action.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.il(x))\n",
    "        x = F.relu(self.h1(x))\n",
    "        x = F.relu(self.h2(x))\n",
    "        # x = F.relu(self.h3(x))\n",
    "        x = self.ol(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c165014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for training agent (expert)\n",
    "\n",
    "class DQNTrainer:\n",
    "        \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    GAMMA = 0.999\n",
    "    EPS_START = 0.95\n",
    "    num_episodes = 200\n",
    "    EPS_END = 0.05\n",
    "    EPS_DECAY = num_episodes * 0.9\n",
    "    TARGET_UPDATE = 10\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(40, interpolation=Image.BICUBIC),\n",
    "                        T.ToTensor()])\n",
    "\n",
    "    def __init__(self, args, env, name):\n",
    "        \n",
    "        save_path = f'../Results/DQN/{name}/'\n",
    "        pathlib.Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.env = gym.wrappers.Monitor(env, save_path, video_callable=lambda episode_id: episode_id % 199 == 0, \n",
    "                                        force=True)\n",
    "        # records a video for every 199th episode\n",
    "        self.env.reset()\n",
    "\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.is_trained = False\n",
    "        self.avg_feature = None\n",
    "\n",
    "        if args.config_str is not None:     # load pretrained model if given\n",
    "            self.is_trained = True\n",
    "            pth = os.path.abspath(args.config_str)\n",
    "            assert pathlib.Path(pth).exists()\n",
    "            data = torch.load(pth)\n",
    "            self.policy_net.load_state_dict(data['mdl'])\n",
    "            self.avg_feature = data.get('avgFeat')\n",
    "            print('LOADED MODEL')\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.best_model = None\n",
    "        self.best_rwd = -float('inf')\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "\n",
    "        self.NUM_UPDATE = 1\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        self.plot = args.plot\n",
    "        self.name = name\n",
    "        plt.ion()\n",
    "\n",
    "        if self.plot:\n",
    "            plt.figure()\n",
    "            self.init_screen = self.get_screen()\n",
    "            plt.imshow(self.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "            plt.title('Example Extracted Screen')\n",
    "\n",
    "\n",
    "    def get_cart_location(self, screen_width):\n",
    "\n",
    "        world_width = self.env.unwrapped.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        # horizontal position of the cart's center on the screen\n",
    "        cart_position_on_screen = int(self.env.unwrapped.state[0] * scale + screen_width / 2.0)\n",
    "\n",
    "        return cart_position_on_screen\n",
    "\n",
    "    \n",
    "    def get_screen(self):\n",
    "        \n",
    "        # Get the screen from the environment and transpose it into torch order (CHW)\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        top_cutoff, bottom_cutoff = int(screen_height * 0.4), int(screen_height * 0.8)\n",
    "        screen = screen[:, top_cutoff:bottom_cutoff]\n",
    "\n",
    "        # position of the cart on the screen\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = self.get_cart_location(screen_width)\n",
    "\n",
    "        # define the range for extracting a square image centered on the cart\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2)\n",
    "\n",
    "        # Extract the square image centered on the cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "\n",
    "        # Resize, and add a batch dimension (NCHW)\n",
    "        return self.resize(screen).unsqueeze(0).to(self.device)\n",
    "    \n",
    "\n",
    "    def select_action(self, state):\n",
    "        \n",
    "        sample = random.random()\n",
    "        \n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        \n",
    "        self.steps_done += 1\n",
    "\n",
    "        if sample > eps_threshold:\n",
    "            # Exploitation: Choose the action with the larger expected reward\n",
    "            with torch.inference_mode():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)   # second column on max result is index of where max element was\n",
    "        else:\n",
    "            # Exploration: Choose a random action\n",
    "            return torch.tensor([[random.randrange(2)]], device=self.device, dtype=torch.long)\n",
    "\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        \n",
    "        # Check if there are enough samples in the replay memory\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Perform multiple updates to the model\n",
    "        for i in range(self.NUM_UPDATE):\n",
    "            \n",
    "            # Sample a batch of transitions from the replay memory\n",
    "            transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "            \n",
    "            # Transpose the batch of transitions - converts batch-array of Transitions to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # Create a mask for non-final states and concatenate batch elements\n",
    "            # (a final state would've been the one after which simulation ended)\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                   batch.next_state)), device=self.device, dtype=torch.uint8)\n",
    "            \n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            \n",
    "            state_batch = torch.cat([s.to(self.device) for s in batch.state])\n",
    "            action_batch = torch.cat([a.to(self.device) for a in batch.action])\n",
    "            reward_batch = torch.cat([r.to(self.device) for r in batch.reward])\n",
    "\n",
    "\n",
    "            # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "            # columns of actions taken. These are the actions which would've been taken\n",
    "            # for each batch state according to policy_net\n",
    "            # Q values for the current state-action pairs using the current policy network\n",
    "            state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            # Expected values of actions for non_final_next_states are computed based\n",
    "            # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "            # This is merged based on the mask, such that we'll have either the expected\n",
    "            # state value or 0 in case the state was final.\n",
    "            next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "            # Compute the expected Q values for the current state-action pairs using bellman equation\n",
    "            expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "            # Compute Huber loss - smooth L1 loss function. \n",
    "            # It measures the difference between the predicted Q values and the expected Q values\n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "            # Optimize the model\n",
    "            # sets the gradients to zero, backpropagates the loss, applies gradient clipping \n",
    "            # and updates the model's parameters using the optimizer\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in self.policy_net.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            \n",
    "    def testModel(self, mdl, save_states=False):\n",
    "        \n",
    "        ep_rwd = 0  # Initialize total episode reward\n",
    "        state_list = []  # List to store feature vectors if save_states is True\n",
    "\n",
    "        # Reset the environment and obtain the initial state\n",
    "        state_tp = self.env.reset()\n",
    "        state = torch.from_numpy(state_tp).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "\n",
    "        # Save the initial state's feature vector if save_states is True\n",
    "        if save_states:\n",
    "            state_list.append(self.featurefn(state_tp))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for t in count():\n",
    "                # Select an action using the provided model\n",
    "                a = mdl(state).max(1)[1].view(1, 1)\n",
    "\n",
    "                # Take a step in the environment\n",
    "                state_tp, reward, done, _ = self.env.step(a.item())\n",
    "\n",
    "                # Update the current state\n",
    "                state = torch.from_numpy(state_tp).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "\n",
    "                # Save the feature vector of the current state if save_states is True\n",
    "                if save_states:\n",
    "                    state_list.append(self.featurefn(state_tp))\n",
    "\n",
    "                # Update the total episode reward\n",
    "                ep_rwd += reward\n",
    "\n",
    "                # Break if the episode is done or exceeds a maximum time step\n",
    "                if done or t > 30000:\n",
    "                    break\n",
    "\n",
    "        # Based on the total reward for the episode, determine the best model\n",
    "        if ep_rwd > self.best_rwd and not save_states:\n",
    "            self.best_rwd = ep_rwd\n",
    "            self.best_model = copy.deepcopy(mdl)\n",
    "\n",
    "        # Return the total episode reward and the list of feature vectors if save_states is True\n",
    "        if not save_states:\n",
    "            return ep_rwd\n",
    "        else:\n",
    "            return ep_rwd, state_list\n",
    "\n",
    "\n",
    "    def featurefn(self, state):\n",
    "        \n",
    "        # Normalize state components\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        x = (x + self.env.unwrapped.x_threshold) / (2 * self.env.unwrapped.x_threshold)\n",
    "        x_dot = (x_dot + self.env.unwrapped.x_threshold) / (2 * self.env.unwrapped.x_threshold)\n",
    "        theta = (theta + self.env.unwrapped.theta_threshold_radians) / (2 * self.env.unwrapped.theta_threshold_radians)\n",
    "        theta_dot = (theta_dot + self.env.unwrapped.theta_threshold_radians) / (2 * self.env.unwrapped.theta_threshold_radians)\n",
    "\n",
    "        # Construct feature vector\n",
    "        feat = torch.tensor([\n",
    "            x, x_dot, theta, theta_dot,\n",
    "            x ** 2, x_dot ** 2, theta ** 2, theta_dot ** 2,\n",
    "        ])\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def train(self, rwd_weight=None):\n",
    "        \n",
    "        for i_episode in range(self.num_episodes):\n",
    "            \n",
    "            # Initialize the environment and state\n",
    "            state = torch.from_numpy(self.env.reset()).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "            \n",
    "            for t in count():\n",
    "                # Select and perform an action\n",
    "                action = self.select_action(state)\n",
    "                next_state_np, reward, done, _ = self.env.step(action.item())\n",
    "\n",
    "                # Optionally visualize the screen (if plotting is enabled)\n",
    "                if self.plot and i_episode % 100 == 0:\n",
    "                    self.get_screen()\n",
    "\n",
    "                # Prepare next state\n",
    "                next_state = torch.from_numpy(next_state_np).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "\n",
    "                # Adjust the reward based on the reward weight or environment-specific criteria\n",
    "                if rwd_weight is None:\n",
    "                    reward = torch.tensor([reward], device=self.device)\n",
    "                    x, x_dot, theta, theta_dot = next_state_np   # normalization\n",
    "                    r1 = (self.env.unwrapped.x_threshold - abs(x)) / self.env.unwrapped.x_threshold - 0.8\n",
    "                    r2 = (self.env.unwrapped.theta_threshold_radians - abs(theta)) / self.env.unwrapped.theta_threshold_radians - 0.5\n",
    "                    reward = torch.tensor([r1 + r2])\n",
    "                else:\n",
    "                    feat = self.featurefn(next_state_np)\n",
    "                    reward = rwd_weight.t() @ feat\n",
    "\n",
    "                # Observe new state\n",
    "                if done:\n",
    "                    next_state = None\n",
    "\n",
    "                # Store the transition in self.memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                self.optimize_model()\n",
    "\n",
    "                # Break if episode is done or exceeds a maximum number of steps\n",
    "                if done or t > 30000:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    self.showProgress(i_episode)\n",
    "                    break\n",
    "\n",
    "            # Do not test the model until at least 100 episodes have passed\n",
    "            policy_rwd = 0\n",
    "            if i_episode > 100:\n",
    "                policy_rwd = self.testModel(self.policy_net)\n",
    "                print('Policy Reward: ', policy_rwd)\n",
    "\n",
    "            # Update the target network weights every TARGET_UPDATE episodes\n",
    "            if i_episode % self.TARGET_UPDATE == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # Done training\n",
    "        print('Complete')\n",
    "        self.is_trained = True\n",
    "        pathlib.Path('../Results/DQN/Plots/').mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(f'../Results/DQN/Plots/Train-{self.name}.png')\n",
    "        if self.plot:\n",
    "            self.env.render()\n",
    "            self.env.close()\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "\n",
    "    def showProgress(self, e_num):\n",
    "        \n",
    "        # Calculate the mean duration of the last 100 episodes\n",
    "        means = 0\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "\n",
    "        if len(self.episode_durations) >= 100:\n",
    "            means = durations_t[-100:-1].mean().item()\n",
    "\n",
    "        # Print information about the current episode\n",
    "        info_str = f'Episode {e_num}/{self.num_episodes} -- Duration: {durations_t[-1]} -- AVG: {means}'\n",
    "        print(info_str)\n",
    "\n",
    "        # Plot the episode durations\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        plt.title(f'Performance: {self.name}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "\n",
    "        # If plotting is enabled, also plot the rolling average of durations\n",
    "        if self.plot:\n",
    "            if len(durations_t) >= 100:\n",
    "                means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "                means = torch.cat((torch.zeros(99), means))\n",
    "                plt.plot(means.numpy())\n",
    "\n",
    "            # Pause to allow plots to be updated\n",
    "            plt.pause(0.001)\n",
    "\n",
    "\n",
    "    def saveBestModel(self):\n",
    "        \n",
    "        # directory to save models\n",
    "        pathlib.Path('../Data/Models/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create a dictionary containing the state_dict of the best model and the average feature\n",
    "        state = {\n",
    "            'mdl': self.best_model.state_dict(),\n",
    "            'avgFeat': self.avgFeature\n",
    "        }\n",
    "\n",
    "        import datetime\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        save_name = f'../Data/Models/model_DATE-{now.isoformat()}.pth.tar'\n",
    "        print(save_name)\n",
    "\n",
    "        # Save the state dictionary\n",
    "        torch.save(state, save_name)\n",
    "\n",
    "    def gatherAverageFeature(self):\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            n_iter = 2000\n",
    "\n",
    "            sample_sum = None\n",
    "            rwd_sum = None\n",
    "\n",
    "            # Iterate through episodes to collect features and rewards\n",
    "            for i in tqdm.tqdm(range(n_iter)):\n",
    "                \n",
    "                # Call the testModel method to get rewards and states\n",
    "                rwd, states = self.testModel(self.best_model, True)\n",
    "\n",
    "                # Calculate the mean feature vector across the episode\n",
    "                episode_mean = torch.stack(states).mean(0)\n",
    "\n",
    "                # Accumulate feature and reward sums\n",
    "                if sample_sum is None:\n",
    "                    sample_sum = episode_mean\n",
    "                    rwd_sum = rwd\n",
    "                else:\n",
    "                    sample_sum += episode_mean\n",
    "                    rwd_sum += rwd\n",
    "\n",
    "            # Calculate the average feature vector and reward over all iterations\n",
    "            sample_sum /= n_iter\n",
    "            rwd_sum /= n_iter\n",
    "\n",
    "            # Print the calculated average feature and reward\n",
    "            print('Average Feature: ',sample_sum)\n",
    "            print('Average Reward: ',rwd_sum)\n",
    "\n",
    "        # Update the class attribute 'avgFeature' with the calculated average feature\n",
    "        self.avgFeature = sample_sum\n",
    "\n",
    "        # Return the calculated average feature and reward\n",
    "        return sample_sum, rwd_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee62d1ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/200 -- Duration: 23.0 -- AVG: 0\n",
      "Episode 1/200 -- Duration: 26.0 -- AVG: 0\n",
      "Episode 2/200 -- Duration: 9.0 -- AVG: 0\n",
      "Episode 3/200 -- Duration: 20.0 -- AVG: 0\n",
      "Episode 4/200 -- Duration: 11.0 -- AVG: 0\n",
      "Episode 5/200 -- Duration: 28.0 -- AVG: 0\n",
      "Episode 6/200 -- Duration: 14.0 -- AVG: 0\n",
      "Episode 7/200 -- Duration: 21.0 -- AVG: 0\n",
      "Episode 8/200 -- Duration: 12.0 -- AVG: 0\n",
      "Episode 9/200 -- Duration: 9.0 -- AVG: 0\n",
      "Episode 10/200 -- Duration: 11.0 -- AVG: 0\n",
      "Episode 11/200 -- Duration: 13.0 -- AVG: 0\n",
      "Episode 12/200 -- Duration: 60.0 -- AVG: 0\n",
      "Episode 13/200 -- Duration: 95.0 -- AVG: 0\n",
      "Episode 14/200 -- Duration: 109.0 -- AVG: 0\n",
      "Episode 15/200 -- Duration: 49.0 -- AVG: 0\n",
      "Episode 16/200 -- Duration: 68.0 -- AVG: 0\n",
      "Episode 17/200 -- Duration: 36.0 -- AVG: 0\n",
      "Episode 18/200 -- Duration: 42.0 -- AVG: 0\n",
      "Episode 19/200 -- Duration: 35.0 -- AVG: 0\n",
      "Episode 20/200 -- Duration: 61.0 -- AVG: 0\n",
      "Episode 21/200 -- Duration: 169.0 -- AVG: 0\n",
      "Episode 22/200 -- Duration: 147.0 -- AVG: 0\n",
      "Episode 23/200 -- Duration: 177.0 -- AVG: 0\n",
      "Episode 24/200 -- Duration: 156.0 -- AVG: 0\n",
      "Episode 25/200 -- Duration: 129.0 -- AVG: 0\n",
      "Episode 26/200 -- Duration: 164.0 -- AVG: 0\n",
      "Episode 27/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 28/200 -- Duration: 103.0 -- AVG: 0\n",
      "Episode 29/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 30/200 -- Duration: 180.0 -- AVG: 0\n",
      "Episode 31/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 32/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 33/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 34/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 35/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 36/200 -- Duration: 174.0 -- AVG: 0\n",
      "Episode 37/200 -- Duration: 193.0 -- AVG: 0\n",
      "Episode 38/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 39/200 -- Duration: 138.0 -- AVG: 0\n",
      "Episode 40/200 -- Duration: 172.0 -- AVG: 0\n",
      "Episode 41/200 -- Duration: 198.0 -- AVG: 0\n",
      "Episode 42/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 43/200 -- Duration: 189.0 -- AVG: 0\n",
      "Episode 44/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 45/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 46/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 47/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 48/200 -- Duration: 184.0 -- AVG: 0\n",
      "Episode 49/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 50/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 51/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 52/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 53/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 54/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 55/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 56/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 57/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 58/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 59/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 60/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 61/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 62/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 63/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 64/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 65/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 66/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 67/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 68/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 69/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 70/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 71/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 72/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 73/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 74/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 75/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 76/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 77/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 78/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 79/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 80/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 81/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 82/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 83/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 84/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 85/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 86/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 87/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 88/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 89/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 90/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 91/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 92/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 93/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 94/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 95/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 96/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 97/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 98/200 -- Duration: 200.0 -- AVG: 0\n",
      "Episode 99/200 -- Duration: 200.0 -- AVG: 159.84848022460938\n",
      "Episode 100/200 -- Duration: 200.0 -- AVG: 161.63636779785156\n",
      "Episode 101/200 -- Duration: 200.0 -- AVG: 163.39393615722656\n",
      "Policy Reward:  200.0\n",
      "Episode 102/200 -- Duration: 200.0 -- AVG: 165.32322692871094\n",
      "Policy Reward:  200.0\n",
      "Episode 103/200 -- Duration: 200.0 -- AVG: 167.14141845703125\n",
      "Policy Reward:  200.0\n",
      "Episode 104/200 -- Duration: 200.0 -- AVG: 169.05050659179688\n",
      "Policy Reward:  200.0\n",
      "Episode 105/200 -- Duration: 200.0 -- AVG: 170.78787231445312\n",
      "Policy Reward:  200.0\n",
      "Episode 106/200 -- Duration: 200.0 -- AVG: 172.6666717529297\n",
      "Policy Reward:  200.0\n",
      "Episode 107/200 -- Duration: 200.0 -- AVG: 174.47474670410156\n",
      "Policy Reward:  200.0\n",
      "Episode 108/200 -- Duration: 200.0 -- AVG: 176.3737335205078\n",
      "Policy Reward:  200.0\n",
      "Episode 109/200 -- Duration: 200.0 -- AVG: 178.3030242919922\n",
      "Policy Reward:  200.0\n",
      "Episode 110/200 -- Duration: 200.0 -- AVG: 180.21212768554688\n",
      "Policy Reward:  200.0\n",
      "Episode 111/200 -- Duration: 200.0 -- AVG: 182.10101318359375\n",
      "Policy Reward:  200.0\n",
      "Episode 112/200 -- Duration: 200.0 -- AVG: 183.51515197753906\n",
      "Policy Reward:  200.0\n",
      "Episode 113/200 -- Duration: 200.0 -- AVG: 184.5757598876953\n",
      "Policy Reward:  200.0\n",
      "Episode 114/200 -- Duration: 200.0 -- AVG: 185.4949493408203\n",
      "Policy Reward:  200.0\n",
      "Episode 115/200 -- Duration: 200.0 -- AVG: 187.02020263671875\n",
      "Policy Reward:  200.0\n",
      "Episode 116/200 -- Duration: 200.0 -- AVG: 188.35353088378906\n",
      "Policy Reward:  200.0\n",
      "Episode 117/200 -- Duration: 200.0 -- AVG: 190.01010131835938\n",
      "Policy Reward:  200.0\n",
      "Episode 118/200 -- Duration: 200.0 -- AVG: 191.60606384277344\n",
      "Policy Reward:  200.0\n",
      "Episode 119/200 -- Duration: 200.0 -- AVG: 193.27272033691406\n",
      "Policy Reward:  200.0\n",
      "Episode 120/200 -- Duration: 200.0 -- AVG: 194.67677307128906\n",
      "Policy Reward:  200.0\n",
      "Episode 121/200 -- Duration: 200.0 -- AVG: 194.98989868164062\n",
      "Policy Reward:  200.0\n",
      "Episode 122/200 -- Duration: 200.0 -- AVG: 195.52525329589844\n",
      "Policy Reward:  200.0\n",
      "Episode 123/200 -- Duration: 200.0 -- AVG: 195.757568359375\n",
      "Policy Reward:  200.0\n",
      "Episode 124/200 -- Duration: 200.0 -- AVG: 196.2020263671875\n",
      "Policy Reward:  200.0\n",
      "Episode 125/200 -- Duration: 200.0 -- AVG: 196.919189453125\n",
      "Policy Reward:  200.0\n",
      "Episode 126/200 -- Duration: 200.0 -- AVG: 197.28282165527344\n",
      "Policy Reward:  200.0\n",
      "Episode 127/200 -- Duration: 200.0 -- AVG: 197.28282165527344\n",
      "Policy Reward:  200.0\n",
      "Episode 128/200 -- Duration: 200.0 -- AVG: 198.2626190185547\n",
      "Policy Reward:  200.0\n",
      "Episode 129/200 -- Duration: 200.0 -- AVG: 198.2626190185547\n",
      "Policy Reward:  200.0\n",
      "Episode 130/200 -- Duration: 200.0 -- AVG: 198.4646453857422\n",
      "Policy Reward:  200.0\n",
      "Episode 131/200 -- Duration: 200.0 -- AVG: 198.4646453857422\n",
      "Policy Reward:  200.0\n",
      "Episode 132/200 -- Duration: 200.0 -- AVG: 198.4646453857422\n",
      "Policy Reward:  200.0\n",
      "Episode 133/200 -- Duration: 200.0 -- AVG: 198.4646453857422\n",
      "Policy Reward:  200.0\n",
      "Episode 134/200 -- Duration: 200.0 -- AVG: 198.4646453857422\n",
      "Policy Reward:  200.0\n",
      "Episode 135/200 -- Duration: 200.0 -- AVG: 198.4646453857422\n",
      "Policy Reward:  200.0\n",
      "Episode 136/200 -- Duration: 200.0 -- AVG: 198.72727966308594\n",
      "Policy Reward:  200.0\n",
      "Episode 137/200 -- Duration: 200.0 -- AVG: 198.7979736328125\n",
      "Policy Reward:  200.0\n",
      "Episode 138/200 -- Duration: 200.0 -- AVG: 198.7979736328125\n",
      "Policy Reward:  200.0\n",
      "Episode 139/200 -- Duration: 200.0 -- AVG: 199.4242401123047\n",
      "Policy Reward:  200.0\n",
      "Episode 140/200 -- Duration: 200.0 -- AVG: 199.7070770263672\n",
      "Policy Reward:  200.0\n",
      "Episode 141/200 -- Duration: 200.0 -- AVG: 199.72727966308594\n",
      "Policy Reward:  200.0\n",
      "Episode 142/200 -- Duration: 200.0 -- AVG: 199.72727966308594\n",
      "Policy Reward:  200.0\n",
      "Episode 143/200 -- Duration: 200.0 -- AVG: 199.83837890625\n",
      "Policy Reward:  200.0\n",
      "Episode 144/200 -- Duration: 200.0 -- AVG: 199.83837890625\n",
      "Policy Reward:  200.0\n",
      "Episode 145/200 -- Duration: 200.0 -- AVG: 199.83837890625\n",
      "Policy Reward:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 146/200 -- Duration: 200.0 -- AVG: 199.83837890625\n",
      "Policy Reward:  200.0\n",
      "Episode 147/200 -- Duration: 200.0 -- AVG: 199.83837890625\n",
      "Policy Reward:  200.0\n",
      "Episode 148/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 149/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 150/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 151/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 152/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 153/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 154/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 155/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 156/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 157/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 158/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 159/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 160/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 161/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 162/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 163/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 164/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 165/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 166/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 167/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 168/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 169/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 170/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 171/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 172/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 173/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 174/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 175/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 176/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 177/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 178/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 179/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 180/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 181/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 182/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 183/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 184/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 185/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 186/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 187/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 188/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 189/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 190/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 191/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 192/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 193/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 194/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 195/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 196/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 197/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 198/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Episode 199/200 -- Duration: 200.0 -- AVG: 200.0\n",
      "Policy Reward:  200.0\n",
      "Complete\n",
      "../Data/Models/model_DATE-2024-02-14T16:24:02.644775.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2000/2000 [01:11<00:00, 27.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Feature:  tensor([0.4950, 0.4968, 0.4988, 0.4998, 0.2452, 0.2476, 0.2490, 0.4857],\n",
      "       dtype=torch.float64)\n",
      "Average Reward:  200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.4950, 0.4968, 0.4988, 0.4998, 0.2452, 0.2476, 0.2490, 0.4857],\n",
       "        dtype=torch.float64),\n",
       " 200.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    BATCH_SIZE=128,\n",
    "    GAMMA=0.999,\n",
    "    EPS_START=0.95,\n",
    "    num_episodes=200,\n",
    "    EPS_END=0.05,\n",
    "    EPS_DECAY=200*0.9,\n",
    "    TARGET_UPDATE=10,\n",
    "    config_str=None,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "dqn_trainer = DQNTrainer(env=env, args=args, name='Expert')\n",
    "dqn_trainer.train()\n",
    "dqn_trainer.saveBestModel()\n",
    "dqn_trainer.gatherAverageFeature()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

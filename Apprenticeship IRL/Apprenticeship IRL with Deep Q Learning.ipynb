{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643f3587",
   "metadata": {},
   "source": [
    "# Apprenticeship Learning via IRL - Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495553a8",
   "metadata": {},
   "source": [
    "## Cartpole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080aee0",
   "metadata": {},
   "source": [
    "[CartPole-v0 Wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932e51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import argparse\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import torchvision.transforms as T\n",
    "matplotlib.use(\"TkAgg\")\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae693d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to store the experiences (transitions) of the agent as it interacts with the environment\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity   # maximum number of transitions the buffer can hold\n",
    "        self.memory = []           # list to store the transitions\n",
    "        self.position = 0          # counter to keep track of the next available slot in the buffer\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:                  # if buffer not full, appends the transition\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)        # overwrites the transition at the current position\n",
    "        self.position = (self.position + 1) % self.capacity   # update counter to the next available slot\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4995c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition in the replay memory\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f21d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for Deep Q Learning (DQN)\n",
    "\n",
    "HIDDEN_LAYER = 64\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.il = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.h1 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.h2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.ol = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.il(x))\n",
    "        x = F.relu(self.h1(x))\n",
    "        x = F.relu(self.h2(x))\n",
    "        x = self.ol(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c165014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for training agent (expert)\n",
    "\n",
    "class DQNTrainer:\n",
    "        \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    GAMMA = 0.999\n",
    "    EPS_START = 0.95\n",
    "    num_episodes = 200\n",
    "    EPS_END = 0.05\n",
    "    EPS_DECAY = num_episodes * 0.9\n",
    "    TARGET_UPDATE = 10\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(40, interpolation=Image.BICUBIC),\n",
    "                        T.ToTensor()])\n",
    "\n",
    "    def __init__(self, args, env, name):\n",
    "        \n",
    "        save_path = f'../Results/DQN/{name}/'\n",
    "        pathlib.Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.env = gym.wrappers.Monitor(env, save_path, video_callable=lambda episode_id: episode_id % 199 == 0, \n",
    "                                        force=True)\n",
    "        # records a video for every 199th episode\n",
    "        self.env.reset()\n",
    "\n",
    "        self.policy_net = DQN().to(self.device)\n",
    "        self.target_net = DQN().to(self.device)\n",
    "        self.is_trained = False\n",
    "        self.avg_feature = None\n",
    "\n",
    "        if args.config_str is not None:     # load pretrained model if given\n",
    "            self.is_trained = True\n",
    "            pth = os.path.abspath(args.config_str)\n",
    "            assert pathlib.Path(pth).exists()\n",
    "            data = torch.load(pth)\n",
    "            self.policy_net.load_state_dict(data['mdl'])\n",
    "            self.avg_feature = data.get('avgFeat')\n",
    "            print('LOADED MODEL')\n",
    "\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.best_model = None\n",
    "        self.best_rwd = -float('inf')\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "\n",
    "        self.NUM_UPDATE = 1\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        self.plot = args.plot\n",
    "        self.name = name\n",
    "        plt.ion()\n",
    "\n",
    "        if self.plot:\n",
    "            plt.figure()\n",
    "            self.init_screen = self.get_screen()\n",
    "            plt.imshow(self.get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "            plt.title('Example Extracted Screen')\n",
    "\n",
    "\n",
    "    def get_cart_location(self, screen_width):\n",
    "\n",
    "        world_width = self.env.unwrapped.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        # horizontal position of the cart's center on the screen\n",
    "        cart_position_on_screen = int(self.env.unwrapped.state[0] * scale + screen_width / 2.0)\n",
    "\n",
    "        return cart_position_on_screen\n",
    "\n",
    "    \n",
    "    def get_screen(self):\n",
    "        \n",
    "        # Get the screen from the environment and transpose it into torch order (CHW)\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        top_cutoff, bottom_cutoff = int(screen_height * 0.4), int(screen_height * 0.8)\n",
    "        screen = screen[:, top_cutoff:bottom_cutoff]\n",
    "\n",
    "        # position of the cart on the screen\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = self.get_cart_location(screen_width)\n",
    "\n",
    "        # define the range for extracting a square image centered on the cart\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2)\n",
    "\n",
    "        # Extract the square image centered on the cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "\n",
    "        # Resize, and add a batch dimension (NCHW)\n",
    "        return self.resize(screen).unsqueeze(0).to(self.device)\n",
    "    \n",
    "\n",
    "    def select_action(self, state):\n",
    "        \n",
    "        sample = random.random()\n",
    "        \n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        \n",
    "        self.steps_done += 1\n",
    "\n",
    "        if sample > eps_threshold:\n",
    "            # Exploitation: Choose the action with the larger expected reward\n",
    "            with torch.inference_mode():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "                       # second column on max result is index of where max element is\n",
    "        else:\n",
    "            # Exploration: Choose a random action\n",
    "            return torch.tensor([[random.randrange(2)]], device=self.device, dtype=torch.long)\n",
    "\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        \n",
    "        # Check if there are enough samples in the replay memory\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Perform multiple updates to the model\n",
    "        for i in range(self.NUM_UPDATE):\n",
    "            \n",
    "            # Sample a batch of transitions from the replay memory\n",
    "            transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "            \n",
    "            # Transpose the batch of transitions - converts batch-array of Transitions to Transition of batch-arrays.\n",
    "            # batch = Transition((state1, state2, ...), (action1, action2, ...), (next_state1, next_state2, ...), (reward1, reward2, ...))\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # Create a mask for non-final states and concatenate batch elements\n",
    "            # (a final state would've been the one after which simulation ended)\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                   batch.next_state)), device=self.device, dtype=torch.uint8)\n",
    "            \n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            \n",
    "            state_batch = torch.cat([s.to(self.device) for s in batch.state])\n",
    "            action_batch = torch.cat([a.to(self.device) for a in batch.action])\n",
    "            reward_batch = torch.cat([r.to(self.device) for r in batch.reward])\n",
    "\n",
    "\n",
    "            # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "            # columns of actions taken. These are the actions which would've been taken\n",
    "            # for each batch state according to policy_net\n",
    "            state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            # Expected values of actions for non_final_next_states are computed based\n",
    "            # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "            # This is merged based on the mask, such that we'll have either the expected\n",
    "            # state value or 0 in case the state was final.\n",
    "            next_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "            # Compute the expected Q values for the current state-action pairs using bellman equation\n",
    "            expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "            # Compute Huber loss - smooth L1 loss function. \n",
    "            # It measures the difference between the predicted Q values and the expected Q values\n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "            # Optimize the model\n",
    "            # sets the gradients to zero, backpropagates the loss, applies gradient clipping \n",
    "            # and updates the model's parameters using the optimizer\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in self.policy_net.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            \n",
    "    def testModel(self, mdl, save_states=False):\n",
    "        \n",
    "        ep_rwd = 0  # Initialize total episode reward\n",
    "        state_list = []  # List to store feature vectors if save_states is True\n",
    "\n",
    "        # Reset the environment and obtain the initial state\n",
    "        state_tp = self.env.reset()\n",
    "        state = torch.from_numpy(state_tp).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "\n",
    "        # Save the initial state's feature vector if save_states is True\n",
    "        if save_states:\n",
    "            state_list.append(self.featurefn(state_tp))\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for t in count():\n",
    "                # Select an action using the provided model\n",
    "                a = mdl(state).max(1)[1].view(1, 1)\n",
    "\n",
    "                # Take a step in the environment\n",
    "                state_tp, reward, done, _ = self.env.step(a.item())\n",
    "\n",
    "                # Update the current state\n",
    "                state = torch.from_numpy(state_tp).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "\n",
    "                # Save the feature vector of the current state if save_states is True\n",
    "                if save_states:\n",
    "                    state_list.append(self.featurefn(state_tp))\n",
    "\n",
    "                # Update the total episode reward\n",
    "                ep_rwd += reward\n",
    "\n",
    "                # Break if the episode is done or exceeds a maximum time step\n",
    "                if done or t > 30000:\n",
    "                    break\n",
    "\n",
    "        # Based on the total reward for the episode, determine the best model\n",
    "        if ep_rwd > self.best_rwd and not save_states:\n",
    "            self.best_rwd = ep_rwd\n",
    "            self.best_model = copy.deepcopy(mdl)\n",
    "\n",
    "        # Return the total episode reward and the list of feature vectors if save_states is True\n",
    "        if not save_states:\n",
    "            return ep_rwd\n",
    "        else:\n",
    "            return ep_rwd, state_list\n",
    "\n",
    "\n",
    "    def featurefn(self, state):\n",
    "        \n",
    "        # Normalize state components - horizontal position, velocity, angle of the pole, angular velocity\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        x = (x + self.env.unwrapped.x_threshold) / (2 * self.env.unwrapped.x_threshold)\n",
    "        x_dot = (x_dot + self.env.unwrapped.x_threshold) / (2 * self.env.unwrapped.x_threshold)\n",
    "        theta = (theta + self.env.unwrapped.theta_threshold_radians) / (2 * self.env.unwrapped.theta_threshold_radians)\n",
    "        theta_dot = (theta_dot + self.env.unwrapped.theta_threshold_radians) / (2 * self.env.unwrapped.theta_threshold_radians)\n",
    "\n",
    "        # Construct feature vector\n",
    "        feat = torch.tensor([\n",
    "            x, x_dot, theta, theta_dot,\n",
    "            x ** 2, x_dot ** 2, theta ** 2, theta_dot ** 2,\n",
    "        ])\n",
    "\n",
    "        return feat\n",
    "    \n",
    "\n",
    "    def train(self, rwd_weight=None):\n",
    "        \n",
    "        for i_episode in range(self.num_episodes):\n",
    "            \n",
    "            # Initialize the environment and state\n",
    "            state = torch.from_numpy(self.env.reset()).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "            \n",
    "            for t in count():\n",
    "                # Select and perform an action\n",
    "                action = self.select_action(state)\n",
    "                next_state_np, reward, done, _ = self.env.step(action.item())\n",
    "\n",
    "                # Optionally visualize the screen\n",
    "                if self.plot and i_episode % 100 == 0:\n",
    "                    self.get_screen()\n",
    "\n",
    "                # Prepare next state\n",
    "                next_state = torch.from_numpy(next_state_np).unsqueeze(0).to(self.device, dtype=torch.float)\n",
    "\n",
    "                # Adjust the reward based on the reward weight or environment-specific criteria\n",
    "                if rwd_weight is None:\n",
    "                    reward = torch.tensor([reward], device=self.device)\n",
    "                    x, x_dot, theta, theta_dot = next_state_np   # normalization\n",
    "                    \n",
    "                    # Calculate the normalized distance of the cart's center from the edges of the screen\n",
    "                    # Penalize the agent for being close to the edges, encouraging it to stay away\n",
    "                    r1 = (self.env.unwrapped.x_threshold - abs(x)) / self.env.unwrapped.x_threshold - 0.8\n",
    "                    \n",
    "                    # Calculate the normalized remaining angular range before the pole reaches its maximum angle\n",
    "                    # Penalize the agent for having the pole at a large angle, to keep the pole more upright\n",
    "\n",
    "                    r2 = (self.env.unwrapped.theta_threshold_radians - abs(theta)) / self.env.unwrapped.theta_threshold_radians - 0.5\n",
    "                    \n",
    "                    reward = torch.tensor([r1 + r2])\n",
    "                else:\n",
    "                    feat = self.featurefn(next_state_np)\n",
    "                    reward = rwd_weight.t() @ feat          # wT ⋅ φ\n",
    "\n",
    "                # Observe new state\n",
    "                if done:\n",
    "                    next_state = None\n",
    "\n",
    "                # Store the transition in self.memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                self.optimize_model()\n",
    "\n",
    "                # Break if episode is done or exceeds a maximum number of steps\n",
    "                if done or t > 30000:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    self.showProgress(i_episode)\n",
    "                    break\n",
    "\n",
    "            # Do not test the model until at least 100 episodes have passed\n",
    "            policy_rwd = 0\n",
    "            if i_episode > 100:\n",
    "                policy_rwd = self.testModel(self.policy_net)\n",
    "                print('Policy Reward: ', policy_rwd)\n",
    "\n",
    "            # Update the target network weights every TARGET_UPDATE episodes\n",
    "            if i_episode % self.TARGET_UPDATE == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # Done training\n",
    "        print('\\nTraining Complete\\n')\n",
    "        self.is_trained = True\n",
    "        pathlib.Path('../Results/DQN/Plots/').mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(f'../Results/DQN/Plots/Train-{self.name}.png')\n",
    "        if self.plot:\n",
    "            self.env.render()\n",
    "            self.env.close()\n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "\n",
    "    def showProgress(self, e_num):\n",
    "        \n",
    "        # Calculate the mean duration of the last 100 episodes\n",
    "        means = 0\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "\n",
    "        if len(self.episode_durations) >= 100:\n",
    "            means = durations_t[-100:-1].mean().item()\n",
    "\n",
    "        # Print information about the current episode\n",
    "        info_str = f'Episode {e_num}/{self.num_episodes} -- Duration: {durations_t[-1]} -- AVG: {means}'\n",
    "        print(info_str)\n",
    "\n",
    "        # Plot the episode durations\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        plt.title(f'Performance: {self.name}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "\n",
    "        # If plotting is enabled, also plot the rolling average of durations\n",
    "        if self.plot:\n",
    "            if len(durations_t) >= 100:\n",
    "                means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "                means = torch.cat((torch.zeros(99), means))\n",
    "                plt.plot(means.numpy())\n",
    "\n",
    "            # Pause to allow plots to be updated\n",
    "            plt.pause(0.001)\n",
    "\n",
    "\n",
    "    def saveBestModel(self):\n",
    "        \n",
    "        # directory to save models\n",
    "        pathlib.Path('../Data/Models/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create a dictionary containing the state_dict of the best model and the average feature\n",
    "        state = {\n",
    "            'mdl': self.best_model.state_dict(),\n",
    "            'avgFeat': self.avgFeature\n",
    "        }\n",
    "\n",
    "        import datetime\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        save_name = f'../Data/Models/model_DATE-{now.isoformat()}.pth.tar'\n",
    "        print(save_name)\n",
    "\n",
    "        # Save the state dictionary\n",
    "        torch.save(state, save_name)\n",
    "\n",
    "    def gatherAverageFeature(self):\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            n_iter = 2000\n",
    "\n",
    "            sample_sum = None\n",
    "            rwd_sum = None\n",
    "\n",
    "            # Iterate through episodes to collect features and rewards\n",
    "            for i in tqdm.tqdm(range(n_iter)):\n",
    "                \n",
    "                # Call the testModel method to get rewards and states\n",
    "                rwd, states = self.testModel(self.best_model, save_states=True)\n",
    "\n",
    "                # Calculate the mean feature vector across the episode\n",
    "                episode_mean = torch.stack(states).mean(0)\n",
    "\n",
    "                # Accumulate feature and reward sums\n",
    "                if sample_sum is None:\n",
    "                    sample_sum = episode_mean\n",
    "                    rwd_sum = rwd\n",
    "                else:\n",
    "                    sample_sum += episode_mean\n",
    "                    rwd_sum += rwd\n",
    "\n",
    "            # Calculate the average feature vector and reward over all iterations\n",
    "            sample_sum /= n_iter\n",
    "            rwd_sum /= n_iter\n",
    "\n",
    "            # Print the calculated average feature and reward\n",
    "            print('Average Feature Vector: ',sample_sum)\n",
    "            print('Average Reward: ',rwd_sum)\n",
    "\n",
    "        # Update the class attribute 'avgFeature' with the calculated average feature\n",
    "        self.avgFeature = sample_sum\n",
    "\n",
    "        # Return the calculated average feature and reward\n",
    "        return sample_sum, rwd_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee62d1ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "args = argparse.Namespace(config_str=None, plot=True)\n",
    "\n",
    "dqn_trainer = DQNTrainer(env=env, args=args, name='Expert')\n",
    "dqn_trainer.train()\n",
    "dqn_trainer.saveBestModel()\n",
    "dqn_trainer.gatherAverageFeature()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

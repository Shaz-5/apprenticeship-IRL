{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643f3587",
   "metadata": {},
   "source": [
    "# Apprenticeship Learning via IRL - Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495553a8",
   "metadata": {},
   "source": [
    "## Cartpole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080aee0",
   "metadata": {},
   "source": [
    "[CartPole-v0 Wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932e51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae693d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to store the experiences (transitions) of the agent as it interacts with the environment\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity   # maximum number of transitions the buffer can hold\n",
    "        self.memory = []           # list to store the transitions\n",
    "        self.position = 0          # counter to keep track of the next available slot in the buffer\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Save a transition in the replay memory.\n",
    "\n",
    "        Parameters:\n",
    "        - *args: Tuple representing a transition.\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(self.memory) < self.capacity:                  # if buffer not full, appends the transition\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)        # overwrites the transition at the current position\n",
    "        self.position = (self.position + 1) % self.capacity   # update counter to the next available slot\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the replay memory.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the replay memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4995c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition in the replay memory\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f21d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for Deep Q Learning \n",
    "\n",
    "HIDDEN_LAYER = 64\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DQN model.\n",
    "\n",
    "        The model consists of three hidden layers with ReLU activation functions.\n",
    "\n",
    "        Input:\n",
    "        - 4 input features (state space)\n",
    "        - HIDDEN_LAYER number of neurons in each hidden layer\n",
    "        \n",
    "        Output:\n",
    "          - 2 output neurons representing Q-values for each action\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.il = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.h1 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.h2 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        # self.h3 = nn.Linear(HIDDEN_LAYER, HIDDEN_LAYER)\n",
    "        self.ol = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input state.\n",
    "\n",
    "        Returns:\n",
    "        - Output Q-values for each action.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.il(x))\n",
    "        x = F.relu(self.h1(x))\n",
    "        x = F.relu(self.h2(x))\n",
    "        # x = F.relu(self.h3(x))\n",
    "        x = self.ol(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce73a78",
   "metadata": {},
   "source": [
    "# Apprenticeship Learning via IRL - Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09322083",
   "metadata": {},
   "source": [
    "* This notebook aims to showcase the application of Inverse Reinforcement Learning (IRL) to solve the CartPole model from the OpenAI Gym.\n",
    "\n",
    "* Algorithm based on the paper titled \"Apprenticeship Learning via Inverse Reinforcement Learning.\"\n",
    "\n",
    "* The IRL algorithm requires an expert demonstration as input. In this case, the expert demonstration is derived from a traditional Q-learning implementation (with discretized state space).\n",
    "\n",
    "* The primary purpose of this implementation is to serve as a demonstration, showcasing the effectiveness of Inverse Reinforcement Learning in solving the CartPole problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489168eb",
   "metadata": {},
   "source": [
    "## CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f553f",
   "metadata": {},
   "source": [
    "[CartPole-v0 Wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b09e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1cd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "nbins = 10    # discretization\n",
    "GAMMA = 0.9   # discount factor\n",
    "ALPHA = 0.01  # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26394dc8",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -0.418 rad (-24&deg;) | ~ 0.418 rad (24&deg;)\n",
    "3 | Pole Velocity At Tip | -Inf | Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a53609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the continuous observable state space\n",
    "\n",
    "bins = np.zeros((4,nbins))\n",
    "\n",
    "bins[0] = np.linspace(-2.4, 2.4, nbins)    # position\n",
    "bins[1] = np.linspace(-5, 5, nbins)        # velocity\n",
    "bins[2] = np.linspace(-.418, .418, nbins)  # angle\n",
    "bins[3] = np.linspace(-5, 5, nbins)        # tip velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60fe2057",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# States\n",
    "\n",
    "states = []\n",
    "\n",
    "for i in range (nbins+1):\n",
    "    for j in range (nbins+1):\n",
    "        for k in range(nbins+1):\n",
    "            for l in range(nbins+1):\n",
    "                a=str(i).zfill(2)+str(j).zfill(2)+str(k).zfill(2)+str(l).zfill(2)\n",
    "                states.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0cb3bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14641"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Possible states = (nbins+1)^4\n",
    "\n",
    "len(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df4655",
   "metadata": {},
   "source": [
    "Actions:\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9986c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Possible actions\n",
    "\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bf9fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q Table\n",
    "\n",
    "def init_Q():\n",
    "    \n",
    "    Q = {}\n",
    "    \n",
    "    for state in states:\n",
    "        Q[state] = {}\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "023e6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize observation into bins\n",
    "\n",
    "def assign_bins(observation, bins):\n",
    "    \n",
    "    discretized_state = np.zeros(4)\n",
    "    \n",
    "    for i in range(4):\n",
    "        discretized_state[i] = np.digitize(observation[i], bins[i])\n",
    "        \n",
    "    return discretized_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb870fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode state into string representation as a dictionary\n",
    "\n",
    "def get_string_state(state):\n",
    "\n",
    "    string_state = ''.join(str(int(e)).zfill(2) for e in state)\n",
    "    \n",
    "    return string_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb2d7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an episode\n",
    "\n",
    "def train_an_episode(bins, Q, epsilon=0.5):\n",
    "    \"\"\"\n",
    "    Simulate one episode of training.\n",
    "\n",
    "    Parameters:\n",
    "    - bins: Discretization bins for state representation.\n",
    "    - Q: Q-table for the reinforcement learning agent.\n",
    "    - epsilon: Exploration-exploitation trade-off parameter.\n",
    "\n",
    "    Returns:\n",
    "    - total_reward: Total reward obtained during the episode.\n",
    "    - move_count: Number of moves in the episode.\n",
    "    \"\"\"\n",
    "    \n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    move_count = 0   # no. of moves in an episode\n",
    "    state = get_string_state(assign_bins(observation, bins))\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        move_count += 1\n",
    "\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()  # epsilon-greedy exploration\n",
    "        else:\n",
    "            action = max(Q[state].items(), key=lambda x: x[1])[0]   # action with max value\n",
    "\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # penalize early episode termination\n",
    "        if done and move_count < 200:\n",
    "            reward = -300\n",
    "\n",
    "        new_state = get_string_state(assign_bins(observation, bins))\n",
    "\n",
    "        best_action, max_q_s1a1 = max(Q[new_state].items(), key=lambda x: x[1])\n",
    "        Q[state][action] += ALPHA * (reward + GAMMA * max_q_s1a1 - Q[state][action])\n",
    "        state, action = new_state, best_action\n",
    "\n",
    "    return total_reward, move_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d8d36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for many episodes\n",
    "\n",
    "def train_q_learning(bins, num_episodes=10000, print_interval=1000):\n",
    "    \"\"\"\n",
    "    Train a Q-learning agent through multiple episodes.\n",
    "    \n",
    "    Parameters:\n",
    "    - bins: Discretization bins for state representation.\n",
    "    - num_episodes: Number of training episodes.\n",
    "    - epsilon_decay_factor: Epsilon decay factor for exploration.\n",
    "    - print_interval: Interval for printing training progress.\n",
    "    \n",
    "    Returns:\n",
    "    - episode_lengths: List of lengths for each episode.\n",
    "    - episode_rewards: List of rewards for each episode.\n",
    "    - Q: Trained Q-table.\n",
    "    \"\"\"\n",
    "    \n",
    "    Q = init_Q()\n",
    "    episode_lengths = []\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in tqdm(range(1, num_episodes + 1), desc=\"Training Episodes..\"):\n",
    "        epsilon = 1.0 / np.sqrt(episode + 1)\n",
    "\n",
    "        episode_reward, episode_length = train_an_episode(bins, Q, epsilon)\n",
    "\n",
    "        if episode % print_interval == 0:\n",
    "            print(f\"Episode: {episode}, Epsilon: {epsilon:.4f}, Reward: {episode_reward}\")\n",
    "\n",
    "        episode_lengths.append(episode_length)\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return episode_lengths, episode_rewards, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1747c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward curve\n",
    "\n",
    "def plot_running_avg(total_rewards, title='Running Average Reward', save=False, filename='result'):\n",
    "    \"\"\"\n",
    "    Plot the running average of rewards during training.\n",
    "\n",
    "    Parameters:\n",
    "    - total_rewards: List of total rewards obtained in each episode during training.\n",
    "    - title: Title of the plot.\n",
    "    - save: If True, save the plot as an image file.\n",
    "    - filename: Name of the file if saving the plot.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    num_episodes = len(total_rewards)\n",
    "    running_avg = np.empty(num_episodes)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        running_avg[episode] = np.mean(total_rewards[max(0, episode - 100):(episode + 1)])\n",
    "\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Running Average Reward\")\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f\"{filename}.png\", bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
